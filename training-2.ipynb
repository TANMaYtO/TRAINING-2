{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12946503,"sourceType":"datasetVersion","datasetId":8192880}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T04:16:11.045564Z","iopub.execute_input":"2025-09-17T04:16:11.045852Z","iopub.status.idle":"2025-09-17T04:16:12.640959Z","shell.execute_reply.started":"2025-09-17T04:16:11.045825Z","shell.execute_reply":"2025-09-17T04:16:12.640137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## DAY-1\ndata = []\nn= int(input('number of employees: '))\nfor i in range(n):\n    print(f\"\\nEnter details for employee {i+1}:\")\n    name = input(\"Name: \")\n    age = int(input(\"Age: \"))\n    department = input(\"Department: \")\n    salary = int(input(\"Salary: \"))\n    \n    # append to list as dictionary\n    data.append({\n        \"name\": name,\n        \"age\": age,\n        \"department\": department,\n        \"salary\": salary\n    })           \n\ndf = pd.DataFrame(data)\n\ndf.head(3)\n\ndf[['name', 'salary']]\n\navg_sal = df['salary'].mean()\navg_sal\n\nmax_sal = df['salary'].max()\nprint(df['name'][df['salary'] == max_sal])\n\nnp.std(df['salary'])\n\nage = np.array(df['age'])\ndouble = age*2\ndouble","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:35:08.142764Z","iopub.execute_input":"2025-09-03T05:35:08.143283Z","iopub.status.idle":"2025-09-03T05:35:18.626469Z","shell.execute_reply.started":"2025-09-03T05:35:08.143260Z","shell.execute_reply":"2025-09-03T05:35:18.624717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DAY-3\ndf = pd.read_csv('/kaggle/input/training2-titanic-data/titanic.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:31.084428Z","iopub.execute_input":"2025-09-03T05:36:31.084791Z","iopub.status.idle":"2025-09-03T05:36:31.103607Z","shell.execute_reply.started":"2025-09-03T05:36:31.084766Z","shell.execute_reply":"2025-09-03T05:36:31.102844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:33.243345Z","iopub.execute_input":"2025-09-03T05:36:33.243617Z","iopub.status.idle":"2025-09-03T05:36:33.250354Z","shell.execute_reply.started":"2025-09-03T05:36:33.243598Z","shell.execute_reply":"2025-09-03T05:36:33.249678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Age'] = df['Age'].fillna(df['Age'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:33.413915Z","iopub.execute_input":"2025-09-03T05:36:33.414130Z","iopub.status.idle":"2025-09-03T05:36:33.419156Z","shell.execute_reply.started":"2025-09-03T05:36:33.414115Z","shell.execute_reply":"2025-09-03T05:36:33.418268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:36.523169Z","iopub.execute_input":"2025-09-03T05:36:36.523441Z","iopub.status.idle":"2025-09-03T05:36:36.530360Z","shell.execute_reply.started":"2025-09-03T05:36:36.523420Z","shell.execute_reply":"2025-09-03T05:36:36.529562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['PassengerId', 'Cabin', 'Name', 'Ticket'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:36.655644Z","iopub.execute_input":"2025-09-03T05:36:36.656192Z","iopub.status.idle":"2025-09-03T05:36:36.660212Z","shell.execute_reply.started":"2025-09-03T05:36:36.656175Z","shell.execute_reply":"2025-09-03T05:36:36.659452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:38.493298Z","iopub.execute_input":"2025-09-03T05:36:38.493569Z","iopub.status.idle":"2025-09-03T05:36:38.499317Z","shell.execute_reply.started":"2025-09-03T05:36:38.493548Z","shell.execute_reply":"2025-09-03T05:36:38.498611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:38.717796Z","iopub.execute_input":"2025-09-03T05:36:38.718033Z","iopub.status.idle":"2025-09-03T05:36:38.728527Z","shell.execute_reply.started":"2025-09-03T05:36:38.718018Z","shell.execute_reply":"2025-09-03T05:36:38.727862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:43.957069Z","iopub.execute_input":"2025-09-03T05:36:43.957614Z","iopub.status.idle":"2025-09-03T05:36:43.962993Z","shell.execute_reply.started":"2025-09-03T05:36:43.957587Z","shell.execute_reply":"2025-09-03T05:36:43.962379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode Sex\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:45.840201Z","iopub.execute_input":"2025-09-03T05:36:45.840547Z","iopub.status.idle":"2025-09-03T05:36:45.853738Z","shell.execute_reply.started":"2025-09-03T05:36:45.840521Z","shell.execute_reply":"2025-09-03T05:36:45.852911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:45.974892Z","iopub.execute_input":"2025-09-03T05:36:45.975135Z","iopub.status.idle":"2025-09-03T05:36:45.981799Z","shell.execute_reply.started":"2025-09-03T05:36:45.975118Z","shell.execute_reply":"2025-09-03T05:36:45.981094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:46.113788Z","iopub.execute_input":"2025-09-03T05:36:46.113989Z","iopub.status.idle":"2025-09-03T05:36:46.122531Z","shell.execute_reply.started":"2025-09-03T05:36:46.113974Z","shell.execute_reply":"2025-09-03T05:36:46.121706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:46.253822Z","iopub.execute_input":"2025-09-03T05:36:46.253999Z","iopub.status.idle":"2025-09-03T05:36:46.267040Z","shell.execute_reply.started":"2025-09-03T05:36:46.253986Z","shell.execute_reply":"2025-09-03T05:36:46.266443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked_Q'] = df['Embarked_Q'].astype(int)\ndf['Embarked_S'] = df['Embarked_S'].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.102142Z","iopub.execute_input":"2025-09-03T05:36:49.102430Z","iopub.status.idle":"2025-09-03T05:36:49.107256Z","shell.execute_reply.started":"2025-09-03T05:36:49.102410Z","shell.execute_reply":"2025-09-03T05:36:49.106572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.250851Z","iopub.execute_input":"2025-09-03T05:36:49.251040Z","iopub.status.idle":"2025-09-03T05:36:49.263755Z","shell.execute_reply.started":"2025-09-03T05:36:49.251027Z","shell.execute_reply":"2025-09-03T05:36:49.262998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Survived', axis=1)\ny = df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.404096Z","iopub.execute_input":"2025-09-03T05:36:49.404513Z","iopub.status.idle":"2025-09-03T05:36:49.574067Z","shell.execute_reply.started":"2025-09-03T05:36:49.404496Z","shell.execute_reply":"2025-09-03T05:36:49.573141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Define models in a dictionary\nmodel = RandomForestClassifier(n_estimators=200,random_state = 42)\n# Loop through models\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)    \nacc = accuracy_score(y_test, y_pred)\nprint(f\" Accuracy: {acc:.4f}\")\nprint(classification_report(y_test, y_pred))\nprint(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:52.972191Z","iopub.execute_input":"2025-09-03T05:36:52.972478Z","iopub.status.idle":"2025-09-03T05:37:00.320641Z","shell.execute_reply.started":"2025-09-03T05:36:52.972458Z","shell.execute_reply":"2025-09-03T05:37:00.319793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:37:00.321984Z","iopub.execute_input":"2025-09-03T05:37:00.322960Z","iopub.status.idle":"2025-09-03T05:37:00.337039Z","shell.execute_reply.started":"2025-09-03T05:37:00.322937Z","shell.execute_reply":"2025-09-03T05:37:00.336306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nX_tensor = torch.tensor(X.values, dtype=torch.float32)\ny_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n\nX_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\ntest_dataset = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nclass titanicNN(nn.Module):\n    def __init__(self, inp):\n        super(titanicNN, self).__init__()\n        self.fc1 = nn.Linear(inp, 64)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(64,128)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(128, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu1(self.fc1(x))\n        x = self.relu2(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\nmodel = titanicNN(X.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 1000\nfor EPOCH in range(epochs):\n    model.train()\n    for x_batch , y_batch in train_loader:\n        optimizer.zero_grad()\n        y_pred = model(x_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n\n    if (EPOCH+1)%10 == 0:\n        print(f\"Epoch {EPOCH+1}/{epochs}, Loss: {loss.item():.4f}\")\n\nmodel.eval()\nwith torch.no_grad():\n    y_pred_probs = model(X_test_t)\n    y_pred_labels = (y_pred_probs > 0.5).int()\n    acc = (y_pred_labels.eq(y_test_t.int()).sum().item()) / y_test_t.shape[0]\n    print(f\"Test Accuracy: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:38:56.217891Z","iopub.execute_input":"2025-09-03T05:38:56.218642Z","iopub.status.idle":"2025-09-03T05:39:35.668895Z","shell.execute_reply.started":"2025-09-03T05:38:56.218620Z","shell.execute_reply":"2025-09-03T05:39:35.668054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### DAY-4\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = LogisticRegression(\n        max_iter=1000, solver='lbfgs', C=1.0, random_state=random_state\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'LogisticRegression',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_pred),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= LOGISTIC REGRESSION =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Logistic Regression ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T05:13:22.808761Z","iopub.execute_input":"2025-09-05T05:13:22.809079Z","iopub.status.idle":"2025-09-05T05:13:23.239182Z","shell.execute_reply.started":"2025-09-05T05:13:22.809057Z","shell.execute_reply":"2025-09-05T05:13:23.238090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### DECISION TREES\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = DecisionTreeClassifier(\n        criterion=\"gini\",   # or \"entropy\"\n        max_depth=None,     # try limiting to prevent overfitting\n        random_state=random_state\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'DecisionTree',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_pred),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= DECISION TREE =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Decision Tree ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T04:27:26.403268Z","iopub.execute_input":"2025-09-05T04:27:26.403637Z","iopub.status.idle":"2025-09-05T04:27:27.984153Z","shell.execute_reply.started":"2025-09-05T04:27:26.403611Z","shell.execute_reply":"2025-09-05T04:27:27.983178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### RANDOM FOREST\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = RandomForestClassifier(\n        n_estimators=200,      # number of trees (more trees = better but slower)\n        max_depth=10,          # prevent trees from growing too deep\n        min_samples_split=5,   # minimum samples to split a node\n        min_samples_leaf=2,    # minimum samples per leaf\n        max_features=\"sqrt\",   # best practice for classification\n        bootstrap=True,        # standard RF bootstrapping\n        random_state=random_state,\n        n_jobs=-1              # use all CPU cores\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'RandomForest',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_prob),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= RANDOM FOREST =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Random Forest ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T05:10:02.432606Z","iopub.execute_input":"2025-09-05T05:10:02.433252Z","iopub.status.idle":"2025-09-05T05:10:03.855890Z","shell.execute_reply.started":"2025-09-05T05:10:02.433207Z","shell.execute_reply":"2025-09-05T05:10:03.854769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### SUPPORT VECTOR MACHINE (SVC)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    # Pipeline: StandardScaler -> SVC\n    model = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svc\", SVC(\n            kernel=\"rbf\",        # RBF is the default\n            C=10.0,              # regularization strength (tune this)\n            gamma=\"scale\",       # kernel coefficient\n            probability=True,    # enable predict_proba\n            random_state=random_state\n        ))\n    ])\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'SVC',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_prob),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= SUPPORT VECTOR MACHINE =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('SVM ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-05T06:25:54.088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY 6\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 3)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(8 * 13 * 13, 10)\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = x.view(-1, 8 * 13 * 13)\n        x = self.fc1(x)\n        return x\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrainset = torchvision.datasets.MNIST(\n    root='/kaggle/working',\n    train=True,\n    download=True,\n    transform=transform\n)\ntestset = torchvision.datasets.MNIST(\n    root='/kaggle/working',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SimpleCNN().to(device)\n\nprint(\"Train batches:\", len(trainloader))\nprint(\"Test batches:\", len(testloader))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY 8\n### REGULARIZED CNN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T05:17:14.090144Z","iopub.execute_input":"2025-09-11T05:17:14.090559Z","iopub.status.idle":"2025-09-11T05:17:14.097489Z","shell.execute_reply.started":"2025-09-11T05:17:14.090526Z","shell.execute_reply":"2025-09-11T05:17:14.096007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nclass regCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)   \n        self.bn1 = nn.BatchNorm2d(16)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.5)               \n        self.fc1 = nn.Linear(16 * 14 * 14, 10)\n\n    def forward(self, x):\n        x = self.pool(self.bn1(torch.relu(self.conv1(x))))\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        return self.fc1(x)\n        \ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrainset = torchvision.datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\ntestset = torchvision.datasets.MNIST(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = regCNN().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    scheduler.step()\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(trainloader):.4f}\")\n\nprint(\"Training done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY- 13\n\n# word2vec\n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\n\ncorpus= 'king queen man woman'\nwords= corpus.split()\nvocab= list(set(words))\nword_to_idx= {word: i for i, word in enumerate(vocab)}\nidx_to_word= {i: word for word,i in word_to_idx.items()}\npairs= [(words[i], words[i+1]) for i in range(len(words)-1)]\n\nclass word2vec(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.emb= nn.Embedding(vocab_size, embed_dim)\n        self.linear= nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x):\n        return self.linear(self.emb(x))\n\nmodel = word2vec(len(vocab), 5)\nloss_fn= nn.CrossEntropyLoss()\nopt= optim.SGD(model.parameters(), lr= 1e-5)\nfor epoch in range(50):\n    total_loss= 0\n    for center, context in pairs:\n        context_idx= torch.tensor([word_to_idx[context]])\n        center_idx= torch.tensor([word_to_idx[center]])\n\n        opt.zero_grad()\n        pred= model(center_idx)\n        loss= loss_fn(pred, context_idx)\n        loss.backward()\n        opt.step()\n        total_loss+= loss.item()\n    print(f'EPOCH {epoch}, loss: {total_loss/len(pairs)}')\n\nembeddings= model.emb.weight.data\nprint('vector for king:', embeddings[word_to_idx['king']])\nprint('vector for queen:', embeddings[word_to_idx['queen']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:50:33.780274Z","iopub.execute_input":"2025-09-16T05:50:33.780983Z","iopub.status.idle":"2025-09-16T05:50:33.880217Z","shell.execute_reply.started":"2025-09-16T05:50:33.780956Z","shell.execute_reply":"2025-09-16T05:50:33.879472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###LSTMM\n\nsentences = ['this movie is so ass', 'this movie is so good omg', \n             'i love this movie', 'i hated it']\nlabels = [0, 1, 1, 0]\n\n# Build vocab\nvocab = {w: i+1 for i, w in enumerate(set(' '.join(sentences).split()))}  # +1 so 0 can be used for padding\nvocab_size = len(vocab) + 1\nmaxlen = 6\n\ndef encode(s):\n    ids= [vocab[w] for w in s.split()]\n    return ids[:maxlen] + [0]*D(maxlen- len(ids))\n\nclass LSTMClassifier(nn.Module):","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:06:07.686533Z","iopub.execute_input":"2025-09-16T06:06:07.687361Z","iopub.status.idle":"2025-09-16T06:06:07.694262Z","shell.execute_reply.started":"2025-09-16T06:06:07.687336Z","shell.execute_reply":"2025-09-16T06:06:07.693433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###LSTMMMMM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nsentences = ['this movie is so ass', 'this movie is so good omg', \n             'i love this movie', 'i hated it']\nlabels = [0, 1, 1, 0]\n\nvocab = {w: i+1 for i, w in enumerate(set(' '.join(sentences).split()))}\nvocab_size = len(vocab) + 1\nmaxlen = 6\ndef encode(s):\n    ids = [vocab[w] for w in s.split()]\n    return ids[:maxlen] + [0]*max(0, maxlen - len(ids))\nX = torch.tensor([encode(s) for s in sentences])\ny = torch.tensor(labels)\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, num_classes)\n    \n    def forward(self, x):\n        x = self.embedding(x)               \n        _, (h_n, _) = self.lstm(x)          \n        out = self.fc(h_n.squeeze(0))       \n        return out\n\nmodel = LSTMClassifier(vocab_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nfor epoch in range(20):\n    optimizer.zero_grad()\n    logits = model(X)\n    loss = criterion(logits, y)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 5 == 0:\n        preds = logits.argmax(dim=1)\n        acc = (preds == y).float().mean().item()\n        print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f} | Acc: {acc:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:09:23.299935Z","iopub.execute_input":"2025-09-16T06:09:23.300333Z","iopub.status.idle":"2025-09-16T06:09:23.548247Z","shell.execute_reply.started":"2025-09-16T06:09:23.300314Z","shell.execute_reply":"2025-09-16T06:09:23.547467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##### DAY 14\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef attention(q,k,v):\n    d= q.size(-1)\n    scores = q@k.transpose(-2,-1)/d ** 0.5\n    attn = F.softmax(scores, dim=-1)\n    return attn @ v, attn\n\nq= torch.randn(1,4,8)\nout, attn= attention(q,q,q)\nprint(out.shape, attn.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T05:38:36.119883Z","iopub.execute_input":"2025-09-17T05:38:36.120096Z","iopub.status.idle":"2025-09-17T05:38:40.196273Z","shell.execute_reply.started":"2025-09-17T05:38:36.120072Z","shell.execute_reply":"2025-09-17T05:38:40.195659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### TRANSFORMER\n\nfrom transformers import pipeline\n\nsummarizer = pipeline('summarization', model= 'facebook/bart-large-cnn')\ntext= 'Climate change is accelerating the frequency and intensity of extreme weather events around the world. Rising global temperatures have contributed to longer heatwaves, stronger hurricanes, and more devastating wildfires. According to the Intergovernmental Panel on Climate Change (IPCC), human activities, especially the burning of fossil fuels, are the primary drivers of these changes. Governments are being urged to adopt cleaner energy sources, reduce greenhouse gas emissions, and invest in climate-resilient infrastructure. Without immediate and coordinated action, scientists warn that the impacts on ecosystems, economies, and human health will become increasingly severe and irreversible.'\noutput= summarizer(text, max_length= 40, min_length= 10)[0]['summary_text']\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T05:40:13.011999Z","iopub.execute_input":"2025-09-17T05:40:13.012684Z","iopub.status.idle":"2025-09-17T05:40:22.940813Z","shell.execute_reply.started":"2025-09-17T05:40:13.012658Z","shell.execute_reply":"2025-09-17T05:40:22.940200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###DAY 15\nfrom transformers import MarianMTModel, MarianTokenizer\n\n# Load model and tokenizer\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n\n# English sentences to translate\nsentences = [\n    'good morning',\n    'how are you',\n    'i hate you',\n    'fuck you, you asshole'\n]\n\n# Tokenize the input batch\nbatch = tok(sentences, return_tensors='pt', padding=True)\n\n# Generate translations\ntranslated = model.generate(**batch, max_length=64)\n\n# Decode and print translations\nfor s, t in zip(sentences, translated):\n    print(f'{s} --> {tok.decode(t, skip_special_tokens=True)}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T05:23:31.600720Z","iopub.execute_input":"2025-09-18T05:23:31.601016Z","iopub.status.idle":"2025-09-18T05:23:37.277443Z","shell.execute_reply.started":"2025-09-18T05:23:31.600996Z","shell.execute_reply":"2025-09-18T05:23:37.275522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DAY 15\n\n# Morse code mapping dictionary\nmorse_map = {\n    'a': '.-', 'b': '-...', 'c': '-.-.', 'd': '-..', 'e': '.', \n    'f': '..-.', 'g': '--.', 'h': '....', 'i': '..', 'j': '.---',\n    'k': '-.-', 'l': '.-..', 'm': '--', 'n': '-.', 'o': '---',\n    'p': '.--.', 'q': '--.-', 'r': '.-.', 's': '...', 't': '-',\n    'u': '..-', 'v': '...-', 'w': '.--', 'x': '-..-', 'y': '-.--',\n    'z': '--..', '0': '-----', '1': '.----', '2': '..---', '3': '...--',\n    '4': '....-', '5': '.....', '6': '-....', '7': '--...', '8': '---..',\n    '9': '----.', ' ': '/', ',': '--..--', '.': '.-.-.-', '?': '..--..',\n    '!': '-.-.--'\n}\n\ndef text_to_morse(text):\n    return ' '.join(morse_map.get(c.lower(), '') for c in text)\n\n# English sentences to \"translate\"\nsentences = [\n    'good morning',\n    'how are you',\n    'i hate you',\n    'fuck you, you asshole'\n]\n\n# Translate and print\nfor s in sentences:\n    print(f\"{s} --> {text_to_morse(s)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T05:36:52.824888Z","iopub.execute_input":"2025-09-18T05:36:52.826707Z","iopub.status.idle":"2025-09-18T05:36:52.836652Z","shell.execute_reply.started":"2025-09-18T05:36:52.826658Z","shell.execute_reply":"2025-09-18T05:36:52.835134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DAY 15\n\nfrom transformers import MarianMTModel, MarianTokenizer\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\n\nsentences = [\n    'good morning',\n    'how are you',\n    'i hate you',\n    'fuck you, you asshole'\n]\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch, max_length=64)\nfor s, t in zip(sentences, translated):\n    print(f'{s} --> {tok.decode(t, skip_special_tokens=True)}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T05:44:37.542243Z","iopub.execute_input":"2025-09-18T05:44:37.543112Z","iopub.status.idle":"2025-09-18T05:44:39.331694Z","shell.execute_reply.started":"2025-09-18T05:44:37.543080Z","shell.execute_reply":"2025-09-18T05:44:39.330567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# English to German\nfrom transformers import MarianMTModel, MarianTokenizer\n\nprint(\"==== Normal Translation ====\")\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n\nsentences = [\n    'Good Morning!',\n    'How are you?',\n    'Is your dog okay now?'\n]\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\n\nprint(\"Translating English to German!!\")\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")\n\n\n# English to Hindi\nprint(\"==== Idioms Translation ====\")\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\n\nsentences = [\n    'Good Morning!',\n    'How are you?',\n    'Is your dog okay now?',\n    'This is not your cup of tea',\n    'Now ball is in your court'\n]\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\n\nprint(\"Translating English to Hindi!!\")\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")\n\n\n# Reverse Translation\nprint(\"==== Reverse Translation ====\")\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n\nsentences = [\n    'Good Morning!',\n    'How are you?',\n    'Is your dog okay now?'\n]\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\nprint(\"Translating English to German!!\")\nde  = []\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")\n    de.append(tok.decode(t, skip_special_tokens=True))\n\n\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-de-en')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-de-en')\n\nsentences = de\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\n\nprint(\"Translating German to English!!\")\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T06:09:41.659305Z","iopub.execute_input":"2025-09-18T06:09:41.659784Z","iopub.status.idle":"2025-09-18T06:09:49.909332Z","shell.execute_reply.started":"2025-09-18T06:09:41.659743Z","shell.execute_reply":"2025-09-18T06:09:49.908104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\n\nmodel_name = 'Helsinki-NLP/opus-mt-en-hi' \ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nsentences = [\"i love my dog\"]\nbatch = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\ntranslated = model.generate(**batch)\ndecoded = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\nfinal_translation = \" \".join(decoded)\nfinal_translation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T05:54:48.487489Z","iopub.execute_input":"2025-09-19T05:54:48.487951Z","iopub.status.idle":"2025-09-19T05:54:50.098839Z","shell.execute_reply.started":"2025-09-19T05:54:48.487914Z","shell.execute_reply":"2025-09-19T05:54:50.097224Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'मैं अपने कुत्ते से प्यार'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from datasets import load_dataset\ndataset= load_dataset('Abirate/english_quotes')\nprint(dataset['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T04:50:25.997620Z","iopub.execute_input":"2025-09-24T04:50:25.997882Z","iopub.status.idle":"2025-09-24T04:50:31.594069Z","shell.execute_reply.started":"2025-09-24T04:50:25.997855Z","shell.execute_reply":"2025-09-24T04:50:31.593365Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72a7414316146fba14020d6b4e5ac6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e86d01d798c42b38acf39c1733119c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e18315fd4ec45cf8832da4f2cb51739"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['quote', 'author', 'tags'],\n    num_rows: 2508\n})\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM , AutoTokenizer , TrainingArguments , Trainer\nfrom peft import LoraConfig , get_peft_model\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Abirate/english_quotes\")\n\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\nlora_config = LoraConfig(task_type = 'CAUSAL_LM' , r = 8 , lora_alpha = 16 , lora_dropout = 0.1)\n\nmodel = get_peft_model(model, lora_config)\n\ndef tokenize(batch):\n    return tokenizer(batch['quotes'] , truncation = True , padding = 'max_length' , max_length = 64)\n\ntokenized = dataset.map(tokenize , batched = True)\n\nargs = TrainingArguments(\n    \"gpt2-tune\" ,\n    per_device_train_batch_size = 4 ,\n    num_train_epochs = 1 ,\n    logging_steps = 10\n)\n\ntrainer = Trainer(\n    model = model ,\n    args = args ,\n    train_dataset = tokenized['train'].select(range(200))\n    )\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T04:55:13.293643Z","iopub.execute_input":"2025-09-24T04:55:13.294010Z","iopub.status.idle":"2025-09-24T04:55:49.350604Z","shell.execute_reply.started":"2025-09-24T04:55:13.293985Z","shell.execute_reply":"2025-09-24T04:55:49.349372Z"}},"outputs":[{"name":"stderr","text":"2025-09-24 04:55:29.095371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758689729.310487      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758689729.372438      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59af05d54d934d3abd528250104d3307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f65c096ab3954899ab77bd43954e81d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b2eb3feca964a51bde5cb97f6a05e40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d30fef6c524b54b5b78e68907f5d0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f9e196314e465eaae0bc0355163231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9441e22e61554cc4be89de3415cdc200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23f5a80218644986b6bc76b85c732e04"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29209ae637324b4da51f06a4066fd3ba"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3862983774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quotes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'max_length'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m args = TrainingArguments(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             dataset_dict[split] = dataset.map(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3077\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3079\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3080\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3523\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m                         \u001b[0mnum_examples_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3474\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3475\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3397\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3398\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3399\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3862983774.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quotes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'max_length'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys_to_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'quotes'"],"ename":"KeyError","evalue":"'quotes'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
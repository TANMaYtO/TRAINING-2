{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12946503,"sourceType":"datasetVersion","datasetId":8192880}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T04:04:14.934743Z","iopub.execute_input":"2025-09-16T04:04:14.935921Z","iopub.status.idle":"2025-09-16T04:04:16.986831Z","shell.execute_reply.started":"2025-09-16T04:04:14.935765Z","shell.execute_reply":"2025-09-16T04:04:16.985638Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/training2-titanic-data/titanic.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"## DAY-1\ndata = []\nn= int(input('number of employees: '))\nfor i in range(n):\n    print(f\"\\nEnter details for employee {i+1}:\")\n    name = input(\"Name: \")\n    age = int(input(\"Age: \"))\n    department = input(\"Department: \")\n    salary = int(input(\"Salary: \"))\n    \n    # append to list as dictionary\n    data.append({\n        \"name\": name,\n        \"age\": age,\n        \"department\": department,\n        \"salary\": salary\n    })           \n\ndf = pd.DataFrame(data)\n\ndf.head(3)\n\ndf[['name', 'salary']]\n\navg_sal = df['salary'].mean()\navg_sal\n\nmax_sal = df['salary'].max()\nprint(df['name'][df['salary'] == max_sal])\n\nnp.std(df['salary'])\n\nage = np.array(df['age'])\ndouble = age*2\ndouble","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:35:08.142764Z","iopub.execute_input":"2025-09-03T05:35:08.143283Z","iopub.status.idle":"2025-09-03T05:35:18.626469Z","shell.execute_reply.started":"2025-09-03T05:35:08.143260Z","shell.execute_reply":"2025-09-03T05:35:18.624717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DAY-3\ndf = pd.read_csv('/kaggle/input/training2-titanic-data/titanic.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:31.084428Z","iopub.execute_input":"2025-09-03T05:36:31.084791Z","iopub.status.idle":"2025-09-03T05:36:31.103607Z","shell.execute_reply.started":"2025-09-03T05:36:31.084766Z","shell.execute_reply":"2025-09-03T05:36:31.102844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:33.243345Z","iopub.execute_input":"2025-09-03T05:36:33.243617Z","iopub.status.idle":"2025-09-03T05:36:33.250354Z","shell.execute_reply.started":"2025-09-03T05:36:33.243598Z","shell.execute_reply":"2025-09-03T05:36:33.249678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Age'] = df['Age'].fillna(df['Age'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:33.413915Z","iopub.execute_input":"2025-09-03T05:36:33.414130Z","iopub.status.idle":"2025-09-03T05:36:33.419156Z","shell.execute_reply.started":"2025-09-03T05:36:33.414115Z","shell.execute_reply":"2025-09-03T05:36:33.418268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:36.523169Z","iopub.execute_input":"2025-09-03T05:36:36.523441Z","iopub.status.idle":"2025-09-03T05:36:36.530360Z","shell.execute_reply.started":"2025-09-03T05:36:36.523420Z","shell.execute_reply":"2025-09-03T05:36:36.529562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['PassengerId', 'Cabin', 'Name', 'Ticket'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:36.655644Z","iopub.execute_input":"2025-09-03T05:36:36.656192Z","iopub.status.idle":"2025-09-03T05:36:36.660212Z","shell.execute_reply.started":"2025-09-03T05:36:36.656175Z","shell.execute_reply":"2025-09-03T05:36:36.659452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:38.493298Z","iopub.execute_input":"2025-09-03T05:36:38.493569Z","iopub.status.idle":"2025-09-03T05:36:38.499317Z","shell.execute_reply.started":"2025-09-03T05:36:38.493548Z","shell.execute_reply":"2025-09-03T05:36:38.498611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:38.717796Z","iopub.execute_input":"2025-09-03T05:36:38.718033Z","iopub.status.idle":"2025-09-03T05:36:38.728527Z","shell.execute_reply.started":"2025-09-03T05:36:38.718018Z","shell.execute_reply":"2025-09-03T05:36:38.727862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:43.957069Z","iopub.execute_input":"2025-09-03T05:36:43.957614Z","iopub.status.idle":"2025-09-03T05:36:43.962993Z","shell.execute_reply.started":"2025-09-03T05:36:43.957587Z","shell.execute_reply":"2025-09-03T05:36:43.962379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode Sex\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:45.840201Z","iopub.execute_input":"2025-09-03T05:36:45.840547Z","iopub.status.idle":"2025-09-03T05:36:45.853738Z","shell.execute_reply.started":"2025-09-03T05:36:45.840521Z","shell.execute_reply":"2025-09-03T05:36:45.852911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:45.974892Z","iopub.execute_input":"2025-09-03T05:36:45.975135Z","iopub.status.idle":"2025-09-03T05:36:45.981799Z","shell.execute_reply.started":"2025-09-03T05:36:45.975118Z","shell.execute_reply":"2025-09-03T05:36:45.981094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:46.113788Z","iopub.execute_input":"2025-09-03T05:36:46.113989Z","iopub.status.idle":"2025-09-03T05:36:46.122531Z","shell.execute_reply.started":"2025-09-03T05:36:46.113974Z","shell.execute_reply":"2025-09-03T05:36:46.121706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:46.253822Z","iopub.execute_input":"2025-09-03T05:36:46.253999Z","iopub.status.idle":"2025-09-03T05:36:46.267040Z","shell.execute_reply.started":"2025-09-03T05:36:46.253986Z","shell.execute_reply":"2025-09-03T05:36:46.266443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked_Q'] = df['Embarked_Q'].astype(int)\ndf['Embarked_S'] = df['Embarked_S'].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.102142Z","iopub.execute_input":"2025-09-03T05:36:49.102430Z","iopub.status.idle":"2025-09-03T05:36:49.107256Z","shell.execute_reply.started":"2025-09-03T05:36:49.102410Z","shell.execute_reply":"2025-09-03T05:36:49.106572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.250851Z","iopub.execute_input":"2025-09-03T05:36:49.251040Z","iopub.status.idle":"2025-09-03T05:36:49.263755Z","shell.execute_reply.started":"2025-09-03T05:36:49.251027Z","shell.execute_reply":"2025-09-03T05:36:49.262998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Survived', axis=1)\ny = df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.404096Z","iopub.execute_input":"2025-09-03T05:36:49.404513Z","iopub.status.idle":"2025-09-03T05:36:49.574067Z","shell.execute_reply.started":"2025-09-03T05:36:49.404496Z","shell.execute_reply":"2025-09-03T05:36:49.573141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Define models in a dictionary\nmodel = RandomForestClassifier(n_estimators=200,random_state = 42)\n# Loop through models\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)    \nacc = accuracy_score(y_test, y_pred)\nprint(f\" Accuracy: {acc:.4f}\")\nprint(classification_report(y_test, y_pred))\nprint(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:52.972191Z","iopub.execute_input":"2025-09-03T05:36:52.972478Z","iopub.status.idle":"2025-09-03T05:37:00.320641Z","shell.execute_reply.started":"2025-09-03T05:36:52.972458Z","shell.execute_reply":"2025-09-03T05:37:00.319793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:37:00.321984Z","iopub.execute_input":"2025-09-03T05:37:00.322960Z","iopub.status.idle":"2025-09-03T05:37:00.337039Z","shell.execute_reply.started":"2025-09-03T05:37:00.322937Z","shell.execute_reply":"2025-09-03T05:37:00.336306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nX_tensor = torch.tensor(X.values, dtype=torch.float32)\ny_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n\nX_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\ntest_dataset = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nclass titanicNN(nn.Module):\n    def __init__(self, inp):\n        super(titanicNN, self).__init__()\n        self.fc1 = nn.Linear(inp, 64)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(64,128)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(128, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu1(self.fc1(x))\n        x = self.relu2(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\nmodel = titanicNN(X.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 1000\nfor EPOCH in range(epochs):\n    model.train()\n    for x_batch , y_batch in train_loader:\n        optimizer.zero_grad()\n        y_pred = model(x_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n\n    if (EPOCH+1)%10 == 0:\n        print(f\"Epoch {EPOCH+1}/{epochs}, Loss: {loss.item():.4f}\")\n\nmodel.eval()\nwith torch.no_grad():\n    y_pred_probs = model(X_test_t)\n    y_pred_labels = (y_pred_probs > 0.5).int()\n    acc = (y_pred_labels.eq(y_test_t.int()).sum().item()) / y_test_t.shape[0]\n    print(f\"Test Accuracy: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:38:56.217891Z","iopub.execute_input":"2025-09-03T05:38:56.218642Z","iopub.status.idle":"2025-09-03T05:39:35.668895Z","shell.execute_reply.started":"2025-09-03T05:38:56.218620Z","shell.execute_reply":"2025-09-03T05:39:35.668054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### DAY-4\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = LogisticRegression(\n        max_iter=1000, solver='lbfgs', C=1.0, random_state=random_state\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'LogisticRegression',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_pred),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= LOGISTIC REGRESSION =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Logistic Regression ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T05:13:22.808761Z","iopub.execute_input":"2025-09-05T05:13:22.809079Z","iopub.status.idle":"2025-09-05T05:13:23.239182Z","shell.execute_reply.started":"2025-09-05T05:13:22.809057Z","shell.execute_reply":"2025-09-05T05:13:23.238090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### DECISION TREES\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = DecisionTreeClassifier(\n        criterion=\"gini\",   # or \"entropy\"\n        max_depth=None,     # try limiting to prevent overfitting\n        random_state=random_state\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'DecisionTree',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_pred),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= DECISION TREE =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Decision Tree ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T04:27:26.403268Z","iopub.execute_input":"2025-09-05T04:27:26.403637Z","iopub.status.idle":"2025-09-05T04:27:27.984153Z","shell.execute_reply.started":"2025-09-05T04:27:26.403611Z","shell.execute_reply":"2025-09-05T04:27:27.983178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### RANDOM FOREST\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = RandomForestClassifier(\n        n_estimators=200,      # number of trees (more trees = better but slower)\n        max_depth=10,          # prevent trees from growing too deep\n        min_samples_split=5,   # minimum samples to split a node\n        min_samples_leaf=2,    # minimum samples per leaf\n        max_features=\"sqrt\",   # best practice for classification\n        bootstrap=True,        # standard RF bootstrapping\n        random_state=random_state,\n        n_jobs=-1              # use all CPU cores\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'RandomForest',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_prob),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= RANDOM FOREST =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Random Forest ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T05:10:02.432606Z","iopub.execute_input":"2025-09-05T05:10:02.433252Z","iopub.status.idle":"2025-09-05T05:10:03.855890Z","shell.execute_reply.started":"2025-09-05T05:10:02.433207Z","shell.execute_reply":"2025-09-05T05:10:03.854769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### SUPPORT VECTOR MACHINE (SVC)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    # Pipeline: StandardScaler -> SVC\n    model = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svc\", SVC(\n            kernel=\"rbf\",        # RBF is the default\n            C=10.0,              # regularization strength (tune this)\n            gamma=\"scale\",       # kernel coefficient\n            probability=True,    # enable predict_proba\n            random_state=random_state\n        ))\n    ])\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'SVC',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_prob),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= SUPPORT VECTOR MACHINE =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('SVM ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-05T06:25:54.088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY 6\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 3)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(8 * 13 * 13, 10)\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = x.view(-1, 8 * 13 * 13)\n        x = self.fc1(x)\n        return x\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrainset = torchvision.datasets.MNIST(\n    root='/kaggle/working',\n    train=True,\n    download=True,\n    transform=transform\n)\ntestset = torchvision.datasets.MNIST(\n    root='/kaggle/working',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SimpleCNN().to(device)\n\nprint(\"Train batches:\", len(trainloader))\nprint(\"Test batches:\", len(testloader))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY 8\n### REGULARIZED CNN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T05:17:14.090144Z","iopub.execute_input":"2025-09-11T05:17:14.090559Z","iopub.status.idle":"2025-09-11T05:17:14.097489Z","shell.execute_reply.started":"2025-09-11T05:17:14.090526Z","shell.execute_reply":"2025-09-11T05:17:14.096007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nclass regCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)   \n        self.bn1 = nn.BatchNorm2d(16)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.5)               \n        self.fc1 = nn.Linear(16 * 14 * 14, 10)\n\n    def forward(self, x):\n        x = self.pool(self.bn1(torch.relu(self.conv1(x))))\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        return self.fc1(x)\n        \ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrainset = torchvision.datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\ntestset = torchvision.datasets.MNIST(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = regCNN().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    scheduler.step()\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(trainloader):.4f}\")\n\nprint(\"Training done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY- 13\n\n# word2vec\n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\n\ncorpus= 'king queen man woman'\nwords= corpus.split()\nvocab= list(set(words))\nword_to_idx= {word: i for i, word in enumerate(vocab)}\nidx_to_word= {i: word for word,i in word_to_idx.items()}\npairs= [(words[i], words[i+1]) for i in range(len(words)-1)]\n\nclass word2vec(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.emb= nn.Embedding(vocab_size, embed_dim)\n        self.linear= nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x):\n        return self.linear(self.emb(x))\n\nmodel = word2vec(len(vocab), 5)\nloss_fn= nn.CrossEntropyLoss()\nopt= optim.SGD(model.parameters(), lr= 1e-5)\nfor epoch in range(50):\n    total_loss= 0\n    for center, context in pairs:\n        context_idx= torch.tensor([word_to_idx[context]])\n        center_idx= torch.tensor([word_to_idx[center]])\n\n        opt.zero_grad()\n        pred= model(center_idx)\n        loss= loss_fn(pred, context_idx)\n        loss.backward()\n        opt.step()\n        total_loss+= loss.item()\n    print(f'EPOCH {epoch}, loss: {total_loss/len(pairs)}')\n\nembeddings= model.emb.weight.data\nprint('vector for king:', embeddings[word_to_idx['king']])\nprint('vector for queen:', embeddings[word_to_idx['queen']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:50:33.780274Z","iopub.execute_input":"2025-09-16T05:50:33.780983Z","iopub.status.idle":"2025-09-16T05:50:33.880217Z","shell.execute_reply.started":"2025-09-16T05:50:33.780956Z","shell.execute_reply":"2025-09-16T05:50:33.879472Z"}},"outputs":[{"name":"stdout","text":"EPOCH 0, loss: 1.6513217290242512\nEPOCH 1, loss: 1.6512994368871052\nEPOCH 2, loss: 1.651277224222819\nEPOCH 3, loss: 1.6512549718221028\nEPOCH 4, loss: 1.6512326796849568\nEPOCH 5, loss: 1.6512104272842407\nEPOCH 6, loss: 1.6511882940928142\nEPOCH 7, loss: 1.6511659622192383\nEPOCH 8, loss: 1.6511437892913818\nEPOCH 9, loss: 1.6511214971542358\nEPOCH 10, loss: 1.6510993242263794\nEPOCH 11, loss: 1.6510769923528035\nEPOCH 12, loss: 1.6510547399520874\nEPOCH 13, loss: 1.6510326067606609\nEPOCH 14, loss: 1.651010274887085\nEPOCH 15, loss: 1.6509881019592285\nEPOCH 16, loss: 1.6509658495585124\nEPOCH 17, loss: 1.650943636894226\nEPOCH 18, loss: 1.6509214242299397\nEPOCH 19, loss: 1.650899092356364\nEPOCH 20, loss: 1.6508769989013672\nEPOCH 21, loss: 1.6508547067642212\nEPOCH 22, loss: 1.6508324543635051\nEPOCH 23, loss: 1.6508102019627888\nEPOCH 24, loss: 1.6507879495620728\nEPOCH 25, loss: 1.6507657766342163\nEPOCH 26, loss: 1.6507436037063599\nEPOCH 27, loss: 1.650721271832784\nEPOCH 28, loss: 1.6506991386413574\nEPOCH 29, loss: 1.6506768465042114\nEPOCH 30, loss: 1.6506546139717102\nEPOCH 31, loss: 1.650632381439209\nEPOCH 32, loss: 1.6506101489067078\nEPOCH 33, loss: 1.6505879561106365\nEPOCH 34, loss: 1.6505657037099202\nEPOCH 35, loss: 1.6505434314409893\nEPOCH 36, loss: 1.6505212386449177\nEPOCH 37, loss: 1.6504989663759868\nEPOCH 38, loss: 1.6504768133163452\nEPOCH 39, loss: 1.6504545609156291\nEPOCH 40, loss: 1.6504322091738384\nEPOCH 41, loss: 1.6504100958506267\nEPOCH 42, loss: 1.6503878633181255\nEPOCH 43, loss: 1.6503655910491943\nEPOCH 44, loss: 1.6503433585166931\nEPOCH 45, loss: 1.6503211657206218\nEPOCH 46, loss: 1.6502990126609802\nEPOCH 47, loss: 1.6502767006556194\nEPOCH 48, loss: 1.6502544681231182\nEPOCH 49, loss: 1.6502323349316914\nvector for king: tensor([-0.1652,  0.3786,  0.3052,  0.7745,  0.0865])\nvector for queen: tensor([-1.5707,  0.5008,  0.7482,  0.1447,  0.6871])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
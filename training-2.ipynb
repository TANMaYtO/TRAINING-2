{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12946503,"sourceType":"datasetVersion","datasetId":8192880}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T04:16:11.045564Z","iopub.execute_input":"2025-09-17T04:16:11.045852Z","iopub.status.idle":"2025-09-17T04:16:12.640959Z","shell.execute_reply.started":"2025-09-17T04:16:11.045825Z","shell.execute_reply":"2025-09-17T04:16:12.640137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## DAY-1\ndata = []\nn= int(input('number of employees: '))\nfor i in range(n):\n    print(f\"\\nEnter details for employee {i+1}:\")\n    name = input(\"Name: \")\n    age = int(input(\"Age: \"))\n    department = input(\"Department: \")\n    salary = int(input(\"Salary: \"))\n    \n    # append to list as dictionary\n    data.append({\n        \"name\": name,\n        \"age\": age,\n        \"department\": department,\n        \"salary\": salary\n    })           \n\ndf = pd.DataFrame(data)\n\ndf.head(3)\n\ndf[['name', 'salary']]\n\navg_sal = df['salary'].mean()\navg_sal\n\nmax_sal = df['salary'].max()\nprint(df['name'][df['salary'] == max_sal])\n\nnp.std(df['salary'])\n\nage = np.array(df['age'])\ndouble = age*2\ndouble","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:35:08.142764Z","iopub.execute_input":"2025-09-03T05:35:08.143283Z","iopub.status.idle":"2025-09-03T05:35:18.626469Z","shell.execute_reply.started":"2025-09-03T05:35:08.143260Z","shell.execute_reply":"2025-09-03T05:35:18.624717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DAY-3\ndf = pd.read_csv('/kaggle/input/training2-titanic-data/titanic.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:31.084428Z","iopub.execute_input":"2025-09-03T05:36:31.084791Z","iopub.status.idle":"2025-09-03T05:36:31.103607Z","shell.execute_reply.started":"2025-09-03T05:36:31.084766Z","shell.execute_reply":"2025-09-03T05:36:31.102844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:33.243345Z","iopub.execute_input":"2025-09-03T05:36:33.243617Z","iopub.status.idle":"2025-09-03T05:36:33.250354Z","shell.execute_reply.started":"2025-09-03T05:36:33.243598Z","shell.execute_reply":"2025-09-03T05:36:33.249678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Age'] = df['Age'].fillna(df['Age'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:33.413915Z","iopub.execute_input":"2025-09-03T05:36:33.414130Z","iopub.status.idle":"2025-09-03T05:36:33.419156Z","shell.execute_reply.started":"2025-09-03T05:36:33.414115Z","shell.execute_reply":"2025-09-03T05:36:33.418268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:36.523169Z","iopub.execute_input":"2025-09-03T05:36:36.523441Z","iopub.status.idle":"2025-09-03T05:36:36.530360Z","shell.execute_reply.started":"2025-09-03T05:36:36.523420Z","shell.execute_reply":"2025-09-03T05:36:36.529562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['PassengerId', 'Cabin', 'Name', 'Ticket'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:36.655644Z","iopub.execute_input":"2025-09-03T05:36:36.656192Z","iopub.status.idle":"2025-09-03T05:36:36.660212Z","shell.execute_reply.started":"2025-09-03T05:36:36.656175Z","shell.execute_reply":"2025-09-03T05:36:36.659452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:38.493298Z","iopub.execute_input":"2025-09-03T05:36:38.493569Z","iopub.status.idle":"2025-09-03T05:36:38.499317Z","shell.execute_reply.started":"2025-09-03T05:36:38.493548Z","shell.execute_reply":"2025-09-03T05:36:38.498611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:38.717796Z","iopub.execute_input":"2025-09-03T05:36:38.718033Z","iopub.status.idle":"2025-09-03T05:36:38.728527Z","shell.execute_reply.started":"2025-09-03T05:36:38.718018Z","shell.execute_reply":"2025-09-03T05:36:38.727862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:43.957069Z","iopub.execute_input":"2025-09-03T05:36:43.957614Z","iopub.status.idle":"2025-09-03T05:36:43.962993Z","shell.execute_reply.started":"2025-09-03T05:36:43.957587Z","shell.execute_reply":"2025-09-03T05:36:43.962379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode Sex\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:45.840201Z","iopub.execute_input":"2025-09-03T05:36:45.840547Z","iopub.status.idle":"2025-09-03T05:36:45.853738Z","shell.execute_reply.started":"2025-09-03T05:36:45.840521Z","shell.execute_reply":"2025-09-03T05:36:45.852911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:45.974892Z","iopub.execute_input":"2025-09-03T05:36:45.975135Z","iopub.status.idle":"2025-09-03T05:36:45.981799Z","shell.execute_reply.started":"2025-09-03T05:36:45.975118Z","shell.execute_reply":"2025-09-03T05:36:45.981094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:46.113788Z","iopub.execute_input":"2025-09-03T05:36:46.113989Z","iopub.status.idle":"2025-09-03T05:36:46.122531Z","shell.execute_reply.started":"2025-09-03T05:36:46.113974Z","shell.execute_reply":"2025-09-03T05:36:46.121706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:46.253822Z","iopub.execute_input":"2025-09-03T05:36:46.253999Z","iopub.status.idle":"2025-09-03T05:36:46.267040Z","shell.execute_reply.started":"2025-09-03T05:36:46.253986Z","shell.execute_reply":"2025-09-03T05:36:46.266443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked_Q'] = df['Embarked_Q'].astype(int)\ndf['Embarked_S'] = df['Embarked_S'].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.102142Z","iopub.execute_input":"2025-09-03T05:36:49.102430Z","iopub.status.idle":"2025-09-03T05:36:49.107256Z","shell.execute_reply.started":"2025-09-03T05:36:49.102410Z","shell.execute_reply":"2025-09-03T05:36:49.106572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.250851Z","iopub.execute_input":"2025-09-03T05:36:49.251040Z","iopub.status.idle":"2025-09-03T05:36:49.263755Z","shell.execute_reply.started":"2025-09-03T05:36:49.251027Z","shell.execute_reply":"2025-09-03T05:36:49.262998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Survived', axis=1)\ny = df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:49.404096Z","iopub.execute_input":"2025-09-03T05:36:49.404513Z","iopub.status.idle":"2025-09-03T05:36:49.574067Z","shell.execute_reply.started":"2025-09-03T05:36:49.404496Z","shell.execute_reply":"2025-09-03T05:36:49.573141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Define models in a dictionary\nmodel = RandomForestClassifier(n_estimators=200,random_state = 42)\n# Loop through models\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)    \nacc = accuracy_score(y_test, y_pred)\nprint(f\" Accuracy: {acc:.4f}\")\nprint(classification_report(y_test, y_pred))\nprint(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:36:52.972191Z","iopub.execute_input":"2025-09-03T05:36:52.972478Z","iopub.status.idle":"2025-09-03T05:37:00.320641Z","shell.execute_reply.started":"2025-09-03T05:36:52.972458Z","shell.execute_reply":"2025-09-03T05:37:00.319793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:37:00.321984Z","iopub.execute_input":"2025-09-03T05:37:00.322960Z","iopub.status.idle":"2025-09-03T05:37:00.337039Z","shell.execute_reply.started":"2025-09-03T05:37:00.322937Z","shell.execute_reply":"2025-09-03T05:37:00.336306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nX_tensor = torch.tensor(X.values, dtype=torch.float32)\ny_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n\nX_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\ntest_dataset = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nclass titanicNN(nn.Module):\n    def __init__(self, inp):\n        super(titanicNN, self).__init__()\n        self.fc1 = nn.Linear(inp, 64)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(64,128)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(128, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu1(self.fc1(x))\n        x = self.relu2(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\nmodel = titanicNN(X.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 1000\nfor EPOCH in range(epochs):\n    model.train()\n    for x_batch , y_batch in train_loader:\n        optimizer.zero_grad()\n        y_pred = model(x_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n\n    if (EPOCH+1)%10 == 0:\n        print(f\"Epoch {EPOCH+1}/{epochs}, Loss: {loss.item():.4f}\")\n\nmodel.eval()\nwith torch.no_grad():\n    y_pred_probs = model(X_test_t)\n    y_pred_labels = (y_pred_probs > 0.5).int()\n    acc = (y_pred_labels.eq(y_test_t.int()).sum().item()) / y_test_t.shape[0]\n    print(f\"Test Accuracy: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:38:56.217891Z","iopub.execute_input":"2025-09-03T05:38:56.218642Z","iopub.status.idle":"2025-09-03T05:39:35.668895Z","shell.execute_reply.started":"2025-09-03T05:38:56.218620Z","shell.execute_reply":"2025-09-03T05:39:35.668054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### DAY-4\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = LogisticRegression(\n        max_iter=1000, solver='lbfgs', C=1.0, random_state=random_state\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'LogisticRegression',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_pred),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= LOGISTIC REGRESSION =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Logistic Regression ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T05:13:22.808761Z","iopub.execute_input":"2025-09-05T05:13:22.809079Z","iopub.status.idle":"2025-09-05T05:13:23.239182Z","shell.execute_reply.started":"2025-09-05T05:13:22.809057Z","shell.execute_reply":"2025-09-05T05:13:23.238090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### DECISION TREES\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = DecisionTreeClassifier(\n        criterion=\"gini\",   # or \"entropy\"\n        max_depth=None,     # try limiting to prevent overfitting\n        random_state=random_state\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'DecisionTree',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_pred),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= DECISION TREE =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Decision Tree ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T04:27:26.403268Z","iopub.execute_input":"2025-09-05T04:27:26.403637Z","iopub.status.idle":"2025-09-05T04:27:27.984153Z","shell.execute_reply.started":"2025-09-05T04:27:26.403611Z","shell.execute_reply":"2025-09-05T04:27:27.983178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### RANDOM FOREST\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    model = RandomForestClassifier(\n        n_estimators=200,      # number of trees (more trees = better but slower)\n        max_depth=10,          # prevent trees from growing too deep\n        min_samples_split=5,   # minimum samples to split a node\n        min_samples_leaf=2,    # minimum samples per leaf\n        max_features=\"sqrt\",   # best practice for classification\n        bootstrap=True,        # standard RF bootstrapping\n        random_state=random_state,\n        n_jobs=-1              # use all CPU cores\n    )\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'RandomForest',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_prob),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= RANDOM FOREST =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Random Forest ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T05:10:02.432606Z","iopub.execute_input":"2025-09-05T05:10:02.433252Z","iopub.status.idle":"2025-09-05T05:10:03.855890Z","shell.execute_reply.started":"2025-09-05T05:10:02.433207Z","shell.execute_reply":"2025-09-05T05:10:03.854769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY-5\n### SUPPORT VECTOR MACHINE (SVC)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, roc_auc_score, roc_curve\n)\n\ndef run(show_plot=True, verbose=True, random_state=69):\n    data = load_breast_cancer()\n    X, y = data.data, data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_state, stratify=y\n    )\n\n    # Pipeline: StandardScaler -> SVC\n    model = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svc\", SVC(\n            kernel=\"rbf\",        # RBF is the default\n            C=10.0,              # regularization strength (tune this)\n            gamma=\"scale\",       # kernel coefficient\n            probability=True,    # enable predict_proba\n            random_state=random_state\n        ))\n    ])\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    metrics = {\n        'model': 'SVC',\n        'accuracy': accuracy_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_prob),\n        'confusion_matrix': confusion_matrix(y_test, y_pred),\n        'classification_report': classification_report(y_test, y_pred, digits=4)\n    }\n\n    if verbose:\n        print('======= SUPPORT VECTOR MACHINE =======')\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score: {metrics['f1']:.4f}\")\n        print(f\"AUC: {metrics['auc']:.4f}\")\n        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n        print(\"Classification Report:\\n\", metrics['classification_report'])\n\n    if show_plot:\n        fpr, tpr, _ = roc_curve(y_test, y_prob)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {metrics['auc']:.4f}\")\n        plt.plot([0, 1], [0, 1], '--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('SVM ROC Curve')\n        plt.legend()\n        plt.show()\n\n    return metrics\n\n\nif __name__ == '__main__':\n    run()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-05T06:25:54.088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY 6\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 3)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(8 * 13 * 13, 10)\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = x.view(-1, 8 * 13 * 13)\n        x = self.fc1(x)\n        return x\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrainset = torchvision.datasets.MNIST(\n    root='/kaggle/working',\n    train=True,\n    download=True,\n    transform=transform\n)\ntestset = torchvision.datasets.MNIST(\n    root='/kaggle/working',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SimpleCNN().to(device)\n\nprint(\"Train batches:\", len(trainloader))\nprint(\"Test batches:\", len(testloader))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY 8\n### REGULARIZED CNN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T05:17:14.090144Z","iopub.execute_input":"2025-09-11T05:17:14.090559Z","iopub.status.idle":"2025-09-11T05:17:14.097489Z","shell.execute_reply.started":"2025-09-11T05:17:14.090526Z","shell.execute_reply":"2025-09-11T05:17:14.096007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nclass regCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)   \n        self.bn1 = nn.BatchNorm2d(16)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.5)               \n        self.fc1 = nn.Linear(16 * 14 * 14, 10)\n\n    def forward(self, x):\n        x = self.pool(self.bn1(torch.relu(self.conv1(x))))\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        return self.fc1(x)\n        \ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrainset = torchvision.datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\ntestset = torchvision.datasets.MNIST(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = regCNN().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    scheduler.step()\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(trainloader):.4f}\")\n\nprint(\"Training done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### DAY- 13\n\n# word2vec\n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\n\ncorpus= 'king queen man woman'\nwords= corpus.split()\nvocab= list(set(words))\nword_to_idx= {word: i for i, word in enumerate(vocab)}\nidx_to_word= {i: word for word,i in word_to_idx.items()}\npairs= [(words[i], words[i+1]) for i in range(len(words)-1)]\n\nclass word2vec(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.emb= nn.Embedding(vocab_size, embed_dim)\n        self.linear= nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x):\n        return self.linear(self.emb(x))\n\nmodel = word2vec(len(vocab), 5)\nloss_fn= nn.CrossEntropyLoss()\nopt= optim.SGD(model.parameters(), lr= 1e-5)\nfor epoch in range(50):\n    total_loss= 0\n    for center, context in pairs:\n        context_idx= torch.tensor([word_to_idx[context]])\n        center_idx= torch.tensor([word_to_idx[center]])\n\n        opt.zero_grad()\n        pred= model(center_idx)\n        loss= loss_fn(pred, context_idx)\n        loss.backward()\n        opt.step()\n        total_loss+= loss.item()\n    print(f'EPOCH {epoch}, loss: {total_loss/len(pairs)}')\n\nembeddings= model.emb.weight.data\nprint('vector for king:', embeddings[word_to_idx['king']])\nprint('vector for queen:', embeddings[word_to_idx['queen']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T05:50:33.780274Z","iopub.execute_input":"2025-09-16T05:50:33.780983Z","iopub.status.idle":"2025-09-16T05:50:33.880217Z","shell.execute_reply.started":"2025-09-16T05:50:33.780956Z","shell.execute_reply":"2025-09-16T05:50:33.879472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###LSTMM\n\nsentences = ['this movie is so ass', 'this movie is so good omg', \n             'i love this movie', 'i hated it']\nlabels = [0, 1, 1, 0]\n\n# Build vocab\nvocab = {w: i+1 for i, w in enumerate(set(' '.join(sentences).split()))}  # +1 so 0 can be used for padding\nvocab_size = len(vocab) + 1\nmaxlen = 6\n\ndef encode(s):\n    ids= [vocab[w] for w in s.split()]\n    return ids[:maxlen] + [0]*D(maxlen- len(ids))\n\nclass LSTMClassifier(nn.Module):","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:06:07.686533Z","iopub.execute_input":"2025-09-16T06:06:07.687361Z","iopub.status.idle":"2025-09-16T06:06:07.694262Z","shell.execute_reply.started":"2025-09-16T06:06:07.687336Z","shell.execute_reply":"2025-09-16T06:06:07.693433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###LSTMMMMM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nsentences = ['this movie is so ass', 'this movie is so good omg', \n             'i love this movie', 'i hated it']\nlabels = [0, 1, 1, 0]\n\nvocab = {w: i+1 for i, w in enumerate(set(' '.join(sentences).split()))}\nvocab_size = len(vocab) + 1\nmaxlen = 6\ndef encode(s):\n    ids = [vocab[w] for w in s.split()]\n    return ids[:maxlen] + [0]*max(0, maxlen - len(ids))\nX = torch.tensor([encode(s) for s in sentences])\ny = torch.tensor(labels)\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64, num_classes=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, num_classes)\n    \n    def forward(self, x):\n        x = self.embedding(x)               \n        _, (h_n, _) = self.lstm(x)          \n        out = self.fc(h_n.squeeze(0))       \n        return out\n\nmodel = LSTMClassifier(vocab_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nfor epoch in range(20):\n    optimizer.zero_grad()\n    logits = model(X)\n    loss = criterion(logits, y)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 5 == 0:\n        preds = logits.argmax(dim=1)\n        acc = (preds == y).float().mean().item()\n        print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f} | Acc: {acc:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:09:23.299935Z","iopub.execute_input":"2025-09-16T06:09:23.300333Z","iopub.status.idle":"2025-09-16T06:09:23.548247Z","shell.execute_reply.started":"2025-09-16T06:09:23.300314Z","shell.execute_reply":"2025-09-16T06:09:23.547467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##### DAY 14\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef attention(q,k,v):\n    d= q.size(-1)\n    scores = q@k.transpose(-2,-1)/d ** 0.5\n    attn = F.softmax(scores, dim=-1)\n    return attn @ v, attn\n\nq= torch.randn(1,4,8)\nout, attn= attention(q,q,q)\nprint(out.shape, attn.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T05:38:36.119883Z","iopub.execute_input":"2025-09-17T05:38:36.120096Z","iopub.status.idle":"2025-09-17T05:38:40.196273Z","shell.execute_reply.started":"2025-09-17T05:38:36.120072Z","shell.execute_reply":"2025-09-17T05:38:40.195659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### TRANSFORMER\n\nfrom transformers import pipeline\n\nsummarizer = pipeline('summarization', model= 'facebook/bart-large-cnn')\ntext= 'Climate change is accelerating the frequency and intensity of extreme weather events around the world. Rising global temperatures have contributed to longer heatwaves, stronger hurricanes, and more devastating wildfires. According to the Intergovernmental Panel on Climate Change (IPCC), human activities, especially the burning of fossil fuels, are the primary drivers of these changes. Governments are being urged to adopt cleaner energy sources, reduce greenhouse gas emissions, and invest in climate-resilient infrastructure. Without immediate and coordinated action, scientists warn that the impacts on ecosystems, economies, and human health will become increasingly severe and irreversible.'\noutput= summarizer(text, max_length= 40, min_length= 10)[0]['summary_text']\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T05:40:13.011999Z","iopub.execute_input":"2025-09-17T05:40:13.012684Z","iopub.status.idle":"2025-09-17T05:40:22.940813Z","shell.execute_reply.started":"2025-09-17T05:40:13.012658Z","shell.execute_reply":"2025-09-17T05:40:22.940200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###DAY 15\nfrom transformers import MarianMTModel, MarianTokenizer\n\n# Load model and tokenizer\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n\n# English sentences to translate\nsentences = [\n    'good morning',\n    'how are you',\n    'i hate you',\n    'fuck you, you asshole'\n]\n\n# Tokenize the input batch\nbatch = tok(sentences, return_tensors='pt', padding=True)\n\n# Generate translations\ntranslated = model.generate(**batch, max_length=64)\n\n# Decode and print translations\nfor s, t in zip(sentences, translated):\n    print(f'{s} --> {tok.decode(t, skip_special_tokens=True)}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T05:23:31.600720Z","iopub.execute_input":"2025-09-18T05:23:31.601016Z","iopub.status.idle":"2025-09-18T05:23:37.277443Z","shell.execute_reply.started":"2025-09-18T05:23:31.600996Z","shell.execute_reply":"2025-09-18T05:23:37.275522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DAY 15\n\n# Morse code mapping dictionary\nmorse_map = {\n    'a': '.-', 'b': '-...', 'c': '-.-.', 'd': '-..', 'e': '.', \n    'f': '..-.', 'g': '--.', 'h': '....', 'i': '..', 'j': '.---',\n    'k': '-.-', 'l': '.-..', 'm': '--', 'n': '-.', 'o': '---',\n    'p': '.--.', 'q': '--.-', 'r': '.-.', 's': '...', 't': '-',\n    'u': '..-', 'v': '...-', 'w': '.--', 'x': '-..-', 'y': '-.--',\n    'z': '--..', '0': '-----', '1': '.----', '2': '..---', '3': '...--',\n    '4': '....-', '5': '.....', '6': '-....', '7': '--...', '8': '---..',\n    '9': '----.', ' ': '/', ',': '--..--', '.': '.-.-.-', '?': '..--..',\n    '!': '-.-.--'\n}\n\ndef text_to_morse(text):\n    return ' '.join(morse_map.get(c.lower(), '') for c in text)\n\n# English sentences to \"translate\"\nsentences = [\n    'good morning',\n    'how are you',\n    'i hate you',\n    'fuck you, you asshole'\n]\n\n# Translate and print\nfor s in sentences:\n    print(f\"{s} --> {text_to_morse(s)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T05:36:52.824888Z","iopub.execute_input":"2025-09-18T05:36:52.826707Z","iopub.status.idle":"2025-09-18T05:36:52.836652Z","shell.execute_reply.started":"2025-09-18T05:36:52.826658Z","shell.execute_reply":"2025-09-18T05:36:52.835134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DAY 15\n\nfrom transformers import MarianMTModel, MarianTokenizer\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\n\nsentences = [\n    'good morning',\n    'how are you',\n    'i hate you',\n    'fuck you, you asshole'\n]\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch, max_length=64)\nfor s, t in zip(sentences, translated):\n    print(f'{s} --> {tok.decode(t, skip_special_tokens=True)}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T05:44:37.542243Z","iopub.execute_input":"2025-09-18T05:44:37.543112Z","iopub.status.idle":"2025-09-18T05:44:39.331694Z","shell.execute_reply.started":"2025-09-18T05:44:37.543080Z","shell.execute_reply":"2025-09-18T05:44:39.330567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# English to German\nfrom transformers import MarianMTModel, MarianTokenizer\n\nprint(\"==== Normal Translation ====\")\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n\nsentences = [\n    'Good Morning!',\n    'How are you?',\n    'Is your dog okay now?'\n]\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\n\nprint(\"Translating English to German!!\")\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")\n\n\n# English to Hindi\nprint(\"==== Idioms Translation ====\")\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-hi')\n\nsentences = [\n    'Good Morning!',\n    'How are you?',\n    'Is your dog okay now?',\n    'This is not your cup of tea',\n    'Now ball is in your court'\n]\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\n\nprint(\"Translating English to Hindi!!\")\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")\n\n\n# Reverse Translation\nprint(\"==== Reverse Translation ====\")\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n\nsentences = [\n    'Good Morning!',\n    'How are you?',\n    'Is your dog okay now?'\n]\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\nprint(\"Translating English to German!!\")\nde  = []\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")\n    de.append(tok.decode(t, skip_special_tokens=True))\n\n\nmodel = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-de-en')\ntok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-de-en')\n\nsentences = de\n\nbatch = tok(sentences, return_tensors='pt', padding=True)\ntranslated = model.generate(**batch)\n\nprint(\"Translating German to English!!\")\nfor s, t in zip(sentences, translated):\n    print(f\"{s} -> {tok.decode(t, skip_special_tokens=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T06:09:41.659305Z","iopub.execute_input":"2025-09-18T06:09:41.659784Z","iopub.status.idle":"2025-09-18T06:09:49.909332Z","shell.execute_reply.started":"2025-09-18T06:09:41.659743Z","shell.execute_reply":"2025-09-18T06:09:49.908104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\n\nmodel_name = 'Helsinki-NLP/opus-mt-en-hi' \ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\nsentences = [\"i love my dog\"]\nbatch = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\ntranslated = model.generate(**batch)\ndecoded = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\nfinal_translation = \" \".join(decoded)\nfinal_translation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T05:54:48.487489Z","iopub.execute_input":"2025-09-19T05:54:48.487951Z","iopub.status.idle":"2025-09-19T05:54:50.098839Z","shell.execute_reply.started":"2025-09-19T05:54:48.487914Z","shell.execute_reply":"2025-09-19T05:54:50.097224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\ndataset= load_dataset('Abirate/english_quotes')\nprint(dataset['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T04:50:25.997620Z","iopub.execute_input":"2025-09-24T04:50:25.997882Z","iopub.status.idle":"2025-09-24T04:50:31.594069Z","shell.execute_reply.started":"2025-09-24T04:50:25.997855Z","shell.execute_reply":"2025-09-24T04:50:31.593365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Abirate/english_quotes\")\n\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\nlora_config = LoraConfig(\n    task_type='CAUSAL_LM',\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1\n)\n\nmodel = get_peft_model(model, lora_config)\n\ndef tokenize(batch):\n    encoding = tokenizer(\n        batch['quote'],\n        truncation=True,\n        padding='max_length',\n        max_length=64\n    )\n    encoding['labels'] = encoding['input_ids'].copy()\n    return encoding\n\ntokenized = dataset.map(tokenize, batched=True)\n\nargs = TrainingArguments(\n    output_dir=\"gpt2-tune\",\n    per_device_train_batch_size=4,\n    num_train_epochs=50,\n    logging_steps=10,\n    save_strategy=\"no\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized['train'].select(range(200)),\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T04:06:46.379126Z","iopub.execute_input":"2025-09-25T04:06:46.379386Z","iopub.status.idle":"2025-09-25T04:08:56.544236Z","shell.execute_reply.started":"2025-09-25T04:06:46.379359Z","shell.execute_reply":"2025-09-25T04:08:56.543547Z"}},"outputs":[{"name":"stderr","text":"2025-09-25 04:07:02.755102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758773222.956670      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758773223.012965      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f5d73cf0fec4242a8009abfe20ed3e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3387d48980ec4f2883def6a85f5bf214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55faf03052ee4422afa6a32cd580ae40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0a903ce9634869bc1c9bcd935bd1a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c24c7ef316ca43f5bdee8c953eede85f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dd752ff815849e7bca5b8f23de2d5e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de4920d3e6843f3996d267efd058dc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb9935dc7274be68bfcea9377ac6f94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7e13ccf542a4797a20e630bdfadfce1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7672186f0b144e43815f2b738b468276"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ec8d5c00ec44dc89d18278f8f4f21b"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2814461414.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 01:29, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.389600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.098800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.046800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.946600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.937300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.706800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.621400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.587900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.322700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.132800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.862200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.628300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.391000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.133500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.078400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.988000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.982500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.037400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.950800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.956000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.922600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.016200</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.956500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.951000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.763600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.779800</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.948800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.892700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.843800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.830200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.819700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.775100</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.784000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.841700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.888600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.878100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.824200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.699800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.796700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.881800</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.800600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.925800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.795800</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.755000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.866600</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.796600</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.782600</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.883600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.812400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.822200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.722500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.795500</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.814300</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.801900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.827400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.869100</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.700700</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.856200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.764000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.814000</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.864300</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.724700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.765700</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.759500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.705100</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.713700</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.731800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.781700</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.766000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.816200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.684700</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.713200</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.798900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.726000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.851600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.749000</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.717900</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.728200</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.734600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.631600</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.761900</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.691800</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.780500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.723600</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.837200</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.699200</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.644900</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.783200</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.680100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.632100</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.649900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.728900</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.739000</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.714700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.741800</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.650900</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.767900</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.786100</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.738100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.647300</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.799700</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.674700</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.720400</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.724500</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.693200</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.677700</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.864400</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.684100</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.696600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.680700</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.820800</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.670600</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.747400</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.630000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.818000</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.765500</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.671000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.748700</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.817000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.651900</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.709200</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.750600</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.659500</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.743500</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.708200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1250, training_loss=0.9647347034454345, metrics={'train_runtime': 91.076, 'train_samples_per_second': 109.798, 'train_steps_per_second': 13.725, 'total_flos': 327747502080000.0, 'train_loss': 0.9647347034454345, 'epoch': 50.0})"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\nsentences = [\"the sky is blue.\", \"Dogs are friendly.\", \"Transformers power GPT.\"]\nembeddings = model.encode(sentences, normalize_embeddings = True)\n\nprint(\"Embedding shape:\", embeddings.shape)\n\nindex = faiss.IndexFlatIP(embeddings.shape[1])\nindex.add(embeddings)\nindex.add(np.array(embeddings, dtype=np.float32))\n\nq= 'What powers GPT model ?'\nq_emb= model.encode([q], normalize_embeddings = True)\ndistances, indices = index.search(np.array(claim_embedding, dtype=np.float32), 2)\n[evidence_corpus[i] for i in indices[0]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T05:06:32.174405Z","iopub.execute_input":"2025-09-25T05:06:32.174698Z","iopub.status.idle":"2025-09-25T05:06:32.228888Z","shell.execute_reply.started":"2025-09-25T05:06:32.174672Z","shell.execute_reply":"2025-09-25T05:06:32.227765Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1790694606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"],"ename":"ModuleNotFoundError","evalue":"No module named 'faiss'","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"!pip install faiss-cpu --no-cache-dir\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # to suppress that warning\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T05:08:34.651709Z","iopub.execute_input":"2025-09-25T05:08:34.652038Z","iopub.status.idle":"2025-09-25T05:08:39.965280Z","shell.execute_reply.started":"2025-09-25T05:08:34.652009Z","shell.execute_reply":"2025-09-25T05:08:39.964390Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m240.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.12.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}